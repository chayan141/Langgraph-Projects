{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIvv2brRbZW8XEm2Lc85z9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chayan141/Langgraph-Projects/blob/main/Langgraph_Long_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Term Memory General"
      ],
      "metadata": {
        "id": "NYfyK-IHTZVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Memory : Facts about the users.\n",
        "\n",
        "Episodic Memory : Involves recalling Past Experience or events or acions. It is sometimes implemented through the fewshot prompting.\n",
        "\n",
        "Procedural Memory : Both humans and AI agents remembers set of rules to perform any tasks."
      ],
      "metadata": {
        "id": "7lOtvEmuUEc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qDwTWzimQ42U"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph trustcall langchain_core langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gemini = userdata.get('gemini_api_key')\n",
        "import os\n",
        "os.environ['gemini_api_key'] = gemini"
      ],
      "metadata": {
        "id": "bXOc7cfeYJf_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "9gXvaxieYf4V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash',api_key=gemini)"
      ],
      "metadata": {
        "id": "JBjEP33TYhb8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.config import get_store\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.store.memory import InMemoryStore"
      ],
      "metadata": {
        "id": "OcZm4Z2IYj9g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = InMemoryStore()"
      ],
      "metadata": {
        "id": "0cay5aaUYnJW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TypedDict instance\n",
        "user_profile = {\n",
        "    \"user_name\": \"Lance\",\n",
        "    \"interests\": [\"biking\", \"technology\", \"coffee\"]\n",
        "}\n",
        "user_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe9AUDbmYqkD",
        "outputId": "8837b51d-d4e6-412e-82f0-78bb823e5a10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "namespace_for_memory = (\"1\", \"memory\")\n",
        "\n",
        "key = \"user_profile\"\n",
        "value = user_profile"
      ],
      "metadata": {
        "id": "IRjoZjs9YxxB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.put(namespace_for_memory, key, value)"
      ],
      "metadata": {
        "id": "FJd8g8bMZpKm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in store.search(namespace_for_memory):\n",
        "    print(m.dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC5Yp_UgZsMN",
        "outputId": "cfbcdc66-9429-471a-9fab-f72ae841f790"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['1', 'memory'], 'key': 'user_profile', 'value': {'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}, 'created_at': '2025-07-05T07:50:47.495076+00:00', 'updated_at': '2025-07-05T07:50:47.495082+00:00', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "mtpjoG6uZ1aA",
        "outputId": "baa69222-ad13-43b3-d1c6-83442a6f56ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langgraph.store.base.SearchItem"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langgraph.store.base.SearchItem</b><br/>def __init__(namespace: tuple[str, ...], key: str, value: dict[str, Any], created_at: datetime, updated_at: datetime, score: float | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langgraph/store/base/__init__.py</a>Represents an item returned from a search operation with additional metadata.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 119);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile = store.get(namespace_for_memory, key)\n",
        "profile.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVcKLrinaAi4",
        "outputId": "245e979d-aa51-48db-c02f-d71b153ed5c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Long Term Memory"
      ],
      "metadata": {
        "id": "q9zreMR2bwP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_info(config: RunnableConfig):\n",
        "  \"\"\"Look up user info.\"\"\"\n",
        "    # Same as that provided to `create_react_agent`\n",
        "  store = get_store()\n",
        "  user_id = config['configurable'].get(\"user_id\")\n",
        "  user_info = store.get((\"1\",  \"memory\"), user_id)\n",
        "  return user_info.value if user_info else \"Unknown Users\""
      ],
      "metadata": {
        "id": "CxWJ9t4raKS4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(\n",
        "    model = model,\n",
        "    tools = [get_user_info],\n",
        "    store = store\n",
        ")"
      ],
      "metadata": {
        "id": "mufhhDzXa4sQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n",
        "    config={\"configurable\": {\"user_id\": \"1\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roChRVp6bKNH",
        "outputId": "19fea097-aef5-4e7d-ada2-0cd4922f4545"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='look up user information', additional_kwargs={}, response_metadata={}, id='948ff7f6-0e66-4c48-8805-7b904bdadcd4'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_user_info', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--73e939be-5f0b-4456-8e49-d1198937f9b5-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': '32841e0c-c9a6-43ed-9071-8949bbb4e2c8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 14, 'output_tokens': 5, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Unknown Users', name='get_user_info', id='ff7c255d-984c-4368-b66f-02b96a19743c', tool_call_id='32841e0c-c9a6-43ed-9071-8949bbb4e2c8'),\n",
              "  AIMessage(content='I am unable to retrieve user information at this time. Please try again later.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--28f9b312-8459-424d-8ca2-3a92e50ebc05-0', usage_metadata={'input_tokens': 27, 'output_tokens': 17, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Long Term Memory"
      ],
      "metadata": {
        "id": "YK-pJsQGbziH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "from langgraph.config import get_store\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "class UserProfile(TypedDict):\n",
        "    \"\"\"User profile schema with typed fields\"\"\"\n",
        "    user_name: str  # The user's preferred name\n",
        "    interests: List[str]  # A list of the user's interests"
      ],
      "metadata": {
        "id": "ijQZ2-0vbjiX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_user_info(user_info: UserProfile, config: RunnableConfig):\n",
        "    \"\"\"Save user info.\"\"\"\n",
        "\n",
        "    store = get_store()\n",
        "    user_id = config['configurable'].get(\"user_id\")\n",
        "    store.put((\"2\", \"memory\"), user_id, user_info)\n",
        "    return \"Success\""
      ],
      "metadata": {
        "id": "pBrtI9yHc7H8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(\n",
        "    model = model,\n",
        "    tools = [save_user_info],\n",
        "    store = store\n",
        ")"
      ],
      "metadata": {
        "id": "MM1X0dpQdMfV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith, I have interests in bike riding\"}]},\n",
        "    config={\"configurable\": {\"user_id\": \"user_123\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfZPse1TdMdI",
        "outputId": "32b6809d-1758-4a6b-8425-6eb28c2e3bcc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='My name is John Smith, I have interests in bike riding', additional_kwargs={}, response_metadata={}, id='c1112740-29d5-4ff9-90e3-2b001fdb9252'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_user_info', 'arguments': '{\"user_info\": {\"interests\": [\"bike riding\"], \"user_name\": \"John Smith\"}}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--4f75ac9b-ea70-45ff-900a-090c52f226cd-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_info': {'interests': ['bike riding'], 'user_name': 'John Smith'}}, 'id': '1ddb933d-5a6b-4c86-bb1f-77e327e60524', 'type': 'tool_call'}], usage_metadata={'input_tokens': 46, 'output_tokens': 16, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Success', name='save_user_info', id='92d198a8-42ce-4f25-a560-e84eb084d465', tool_call_id='1ddb933d-5a6b-4c86-bb1f-77e327e60524'),\n",
              "  AIMessage(content=\"OK. I've saved your user information.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--848f1abe-6276-42e5-ae54-39d1414c18d5-0', usage_metadata={'input_tokens': 69, 'output_tokens': 11, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "namespace_for_memory = (\"2\", \"memory\")\n",
        "for m in store.search(namespace_for_memory):\n",
        "    print(m.dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw0YPUupdMap",
        "outputId": "a6ba40ae-1656-4a29-c617-f9f7e5ae7928"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['2', 'memory'], 'key': 'user_123', 'value': {'user_name': 'John Smith', 'interests': ['bike riding']}, 'created_at': '2025-07-05T07:50:49.270852+00:00', 'updated_at': '2025-07-05T07:50:49.270859+00:00', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Schema Processing Using TrustCall"
      ],
      "metadata": {
        "id": "PwhQhN9xewJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complex schemas can be difficult to extract.\n",
        "\n",
        "In addition, updating even simple schemas can pose challenges.\n",
        "\n",
        "Consider our above chatbot.\n",
        "\n",
        "We regenerated the profile schema from scratch each time we chose to save a new memory.\n",
        "\n",
        "This is inefficient, potentially wasting model tokens if the schema contains a lot of information to re-generate each time.\n",
        "\n",
        "Worse, we may loose information when regenerating the profile from scratch.\n",
        "\n",
        "Addressing these problems is the motivation for TrustCall!"
      ],
      "metadata": {
        "id": "KdlddfM4fXm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "# Conversation\n",
        "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"),\n",
        "                AIMessage(content=\"Nice to meet you, Lance.\"),\n",
        "                HumanMessage(content=\"I really like biking around San Francisco.\")]"
      ],
      "metadata": {
        "id": "pA8lVl13dMWV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trustcall import create_extractor\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "\n",
        "# Schema\n",
        "class UserProfile(BaseModel):\n",
        "    \"\"\"User profile schema with typed fields\"\"\"\n",
        "    user_name: str = Field(description=\"The user's preferred name\")\n",
        "    interests: List[str] = Field(description=\"A list of the user's interests\")"
      ],
      "metadata": {
        "id": "IGC-XmoydMTw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the extractor\n",
        "trustcall_extractor = create_extractor(\n",
        "    model,\n",
        "    tools=[UserProfile],\n",
        "    tool_choice=\"UserProfile\"\n",
        ")"
      ],
      "metadata": {
        "id": "b4vCy1a1dMRS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction\n",
        "system_msg = \"Extract the user profile from the following conversation\"\n",
        "\n",
        "# Invoke the extractor\n",
        "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
      ],
      "metadata": {
        "id": "GdQu-oRtdMOv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12300p9dMMJ",
        "outputId": "e6bb6962-6d89-4e25-c341-3c668023d70d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UserProfile (d52863cf-0fc4-4184-a85e-24edbe3bcfab)\n",
            " Call ID: d52863cf-0fc4-4184-a85e-24edbe3bcfab\n",
            "  Args:\n",
            "    interests: ['biking']\n",
            "    user_name: Lance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = result[\"responses\"]\n",
        "schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TWq3FsOdMJZ",
        "outputId": "2b1e765d-566a-415e-f99f-708e23dff03c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserProfile(user_name='Lance', interests=['biking'])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "schema[0].model_dump()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XzqL2SFgPTx",
        "outputId": "281c9530-69c1-4fdc-cefa-1313b2ece7b1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking']}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"response_metadata\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC6kQxHFgTQu",
        "outputId": "254f7c0c-9d54-45d9-9de7-96eeb1c81609"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'd52863cf-0fc4-4184-a85e-24edbe3bcfab'}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the conversation\n",
        "updated_conversation = [HumanMessage(content=\"Hi, I'm Lance.\"),\n",
        "                        AIMessage(content=\"Nice to meet you, Lance.\"),\n",
        "                        HumanMessage(content=\"I really like biking around San Francisco.\"),\n",
        "                        AIMessage(content=\"San Francisco is a great city! Where do you go after biking?\"),\n",
        "                        HumanMessage(content=\"I really like to go to a bakery after biking.\"),]"
      ],
      "metadata": {
        "id": "FNEAWjxPgaoQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the instruction\n",
        "system_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation\"\"\""
      ],
      "metadata": {
        "id": "gc-b2S4Dgepj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
        "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation},\n",
        "                                    {\"existing\": {\"UserProfile\": schema[0].model_dump()}})\n",
        "\n",
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "781zLNe8ghc5",
        "outputId": "27e8c53f-f8ad-4731-9077-fe31049aff84"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UserProfile (c02305eb-f36e-498e-967d-051a48760dca)\n",
            " Call ID: c02305eb-f36e-498e-967d-051a48760dca\n",
            "  Args:\n",
            "    interests: ['biking', 'bakery']\n",
            "    user_name: Lance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"response_metadata\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIg5NtSJgqfv",
        "outputId": "19571f65-afb2-4185-8f58-5377eb66db32"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'c02305eb-f36e-498e-967d-051a48760dca'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_schema = result[\"responses\"][0]\n",
        "updated_schema.model_dump()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfdugaxeguMY",
        "outputId": "99e06716-6e99-40ae-a620-8f0ab47872ae"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'bakery']}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Short Term Memory"
      ],
      "metadata": {
        "id": "zdNsj8GQhuXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With short-term memory enabled, long conversations can exceed the LLM's context window. Common solutions are:\n",
        "\n",
        "1. Trim Messages : Remove first or last N messages (before calling an LLM).\n",
        "\n",
        "2. Delete Messages from langgraph state permanantly.\n",
        "\n",
        "3. Summarize Messages : Summarize earlier messages in the history and replace them with a summary.\n",
        "\n",
        "4. Manage Checkpoints to store and retrieve message history."
      ],
      "metadata": {
        "id": "j27-j4sfiZ1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trim Messages : To trim message history in an agent, use pre_model_hook with the trim_messages function"
      ],
      "metadata": {
        "id": "OBMJN-b6kJ64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
        "from langgraph.graph import StateGraph, START, MessagesState\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "summarization_model = model.bind(max_tokens=128)\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "  messages = trim_messages(\n",
        "      state['messages'],\n",
        "      strategy=\"last\",\n",
        "      token_counter = count_tokens_approximately,\n",
        "      max_tokens=128,\n",
        "      start_on = \"human\",\n",
        "      end_on = (\"human\",\"tool\")\n",
        "  )\n",
        "\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "nRTO88IdjBBN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = InMemorySaver()\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"call_model\",call_model)\n",
        "builder.add_edge(START, \"call_model\")\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "7hRXG875jA-x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
        "\n",
        "final_response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw36mm_RjA8C",
        "outputId": "22125874-654d-47cf-f04b-9a9607071290"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm an AI, so I don't know your name. You haven't told me! You can tell me your name if you'd like. üòä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize Messages"
      ],
      "metadata": {
        "id": "9ZQESBcZl66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langmem --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuTHXJh6mKk6",
        "outputId": "154c1951-adf4-4cf3-8f11-0c551a708db2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/66.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/292.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m286.7/292.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langmem.short_term import SummarizationNode\n",
        "from langchain_core.messages.utils import count_tokens_approximately\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from typing import Any\n",
        "from langchain_core.messages import AnyMessage"
      ],
      "metadata": {
        "id": "01FreL7ljA54"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will keep track of our running summary in the context field (expected by the SummarizationNode)."
      ],
      "metadata": {
        "id": "6q1cDu0ynv0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(MessagesState):\n",
        "    context: dict[str, Any]"
      ],
      "metadata": {
        "id": "C8n9DZz7jA0h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define private state that will be used only for filtering the inputs to call_model node."
      ],
      "metadata": {
        "id": "UsKQnNm7n1La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMInputState(TypedDict):\n",
        "    summarized_messages: list[AnyMessage]\n",
        "    context: dict[str, Any]\n"
      ],
      "metadata": {
        "id": "BRUkEdCbjAyD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_node = SummarizationNode(\n",
        "    token_counter=count_tokens_approximately,\n",
        "    model=summarization_model,\n",
        "    max_tokens=256,\n",
        "    max_tokens_before_summary=256,\n",
        "    max_summary_tokens=128,\n",
        ")"
      ],
      "metadata": {
        "id": "sSXtNIy3jAvg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: LLMInputState):\n",
        "    response = model.invoke(state[\"summarized_messages\"])\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "qubh7ZnIjAs1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = InMemorySaver()\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(call_model)\n",
        "builder.add_node(\"summarize\", summarization_node)\n",
        "builder.add_edge(START, \"summarize\")\n",
        "builder.add_edge(\"summarize\", \"call_model\")\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "BW_IFs70oMfi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the graph\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)"
      ],
      "metadata": {
        "id": "tDo4qcagoPt-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_response[\"messages\"][-1].pretty_print()\n",
        "final_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uw3u8tyoZHU",
        "outputId": "389e2819-9ea2-4825-ba0b-c24e08581feb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. You told me at the beginning of our conversation. üòä\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='a8cc54af-b4bc-41b1-bc5f-4b7f2666745c'),\n",
              "  AIMessage(content=\"Hi Bob! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d71e7a49-66aa-43d1-a9ef-518caf5d78cc-0', usage_metadata={'input_tokens': 6, 'output_tokens': 19, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='e34e8917-4668-44e2-99d6-0de433bd64be'),\n",
              "  AIMessage(content=\"A velvet paw, a silent tread,\\nEmerald eyes within its head.\\nA purring rumble, soft and low,\\nA feline grace in gentle flow.\\n\\nA hunter's instinct, sharp and keen,\\nA sunbeam nap, a sleepy scene.\\nFrom whisker twitch to playful leap,\\nA cat's a secret, buried deep.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--bde2c06a-c7bf-4ea8-af53-9fa92181279f-0', usage_metadata={'input_tokens': 30, 'output_tokens': 75, 'total_tokens': 105, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='6bda933a-059d-44b1-996e-39524a8d5523'),\n",
              "  AIMessage(content=\"A wagging tail, a happy bark,\\nA loyal friend, a light in dark.\\nWith boundless joy and muddy paws,\\nHe breaks the rules and knows no flaws.\\n\\nA wet, cold nose, a loving gaze,\\nThrough fields he runs in sunny haze.\\nA furry cuddle, warm and true,\\nA dog's devotion sees you through.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--b53a3e46-aee2-45a2-9679-4af000f54359-0', usage_metadata={'input_tokens': 111, 'output_tokens': 77, 'total_tokens': 188, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='833973a3-45d0-4435-a895-a7918c7e4431'),\n",
              "  AIMessage(content='Your name is Bob. You told me at the beginning of our conversation. üòä', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d0764bd9-afbc-4b11-9c3a-96ea2b5d8ef6-0', usage_metadata={'input_tokens': 193, 'output_tokens': 17, 'total_tokens': 210, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in final_response[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWy8-mM6ocTK",
        "outputId": "ef649fd8-6271-4e6f-957c-5553ff10c1e5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi, my name is bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! It's nice to meet you. How can I help you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "write a short poem about cats\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "A velvet paw, a silent tread,\n",
            "Emerald eyes within its head.\n",
            "A purring rumble, soft and low,\n",
            "A feline grace in gentle flow.\n",
            "\n",
            "A hunter's instinct, sharp and keen,\n",
            "A sunbeam nap, a sleepy scene.\n",
            "From whisker twitch to playful leap,\n",
            "A cat's a secret, buried deep.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "now do the same but for dogs\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "A wagging tail, a happy bark,\n",
            "A loyal friend, a light in dark.\n",
            "With boundless joy and muddy paws,\n",
            "He breaks the rules and knows no flaws.\n",
            "\n",
            "A wet, cold nose, a loving gaze,\n",
            "Through fields he runs in sunny haze.\n",
            "A furry cuddle, warm and true,\n",
            "A dog's devotion sees you through.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what's my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. You told me at the beginning of our conversation. üòä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context"
      ],
      "metadata": {
        "id": "7Is6NRicpmtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context Engineering is the practice of building dynamic systems that provide the right information and tools in the right format, so that a language model can plausibly accomplish a task.\n",
        "\n",
        "config : data passed at the start of a run.\n",
        "\n",
        "short_term_memory(State)\n",
        "\n",
        "long_term_memory(Store)"
      ],
      "metadata": {
        "id": "tGCXhsjCppaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human In The Loop"
      ],
      "metadata": {
        "id": "UakkNkvpq8oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approve or reject: Pause the graph before a critical step.\n",
        "\n",
        "Edit Graph State: Pause the graph to review and edit the graph state.\n",
        "\n",
        "Reviewing tool calls and validate human input."
      ],
      "metadata": {
        "id": "kOgkd4kxxWfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subgraphs"
      ],
      "metadata": {
        "id": "VdyubqbnleJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shared State Schemas : Parent and subgraph have shared state keys in their state schemas.\n",
        "\n",
        "Different State Schemas: No shared state keys in parent graph and subgraph."
      ],
      "metadata": {
        "id": "mqXG0Y5ylgQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TypedDict is the time hint for dictionaries, no runtime class."
      ],
      "metadata": {
        "id": "cmzfaGj0ygwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define subgraph\n",
        "class SubgraphState(TypedDict):\n",
        "    foo: str\n",
        "    bar: str\n",
        "\n",
        "def subgraph_node_1(state: SubgraphState):\n",
        "    return {\"bar\": \"bar\"}\n",
        "\n",
        "def subgraph_node_2(state: SubgraphState):\n",
        "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
        "    # and is sending update on the shared state key ('foo')\n",
        "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}"
      ],
      "metadata": {
        "id": "0l7R6Q1-pRoV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_builder = StateGraph(SubgraphState)\n",
        "subgraph_builder.add_node(subgraph_node_1)\n",
        "subgraph_builder.add_node(subgraph_node_2)\n",
        "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
        "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
        "subgraph = subgraph_builder.compile()"
      ],
      "metadata": {
        "id": "toatdSeTl9sm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parent graph\n",
        "class ParentState(TypedDict):\n",
        "    foo: str\n",
        "\n",
        "def node_1(state: ParentState):\n",
        "    return {\"foo\": \"hi! \" + state[\"foo\"]}"
      ],
      "metadata": {
        "id": "8xb4wC-UmBhV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(ParentState)\n",
        "builder.add_node(\"node_1\", node_1)\n",
        "builder.add_node(\"node_2\", subgraph)\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "graph = builder.compile()\n",
        "\n",
        "for chunk in graph.stream({\"foo\": \"foo\"}):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg_g7t_rmCua",
        "outputId": "af716907-7369-4c60-d84e-8cb19717c94e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'node_1': {'foo': 'hi! foo'}}\n",
            "{'node_2': {'foo': 'hi! foobar'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subgraph for DIfferent State Schema"
      ],
      "metadata": {
        "id": "KizNRlAnzV20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.state import StateGraph, START\n",
        "\n",
        "# Define subgraph\n",
        "class SubgraphState(TypedDict):\n",
        "    # note that none of these keys are shared with the parent graph state\n",
        "    bar: str\n",
        "    baz: str\n",
        "\n",
        "def subgraph_node_1(state: SubgraphState):\n",
        "    return {\"baz\": \"baz\"}\n",
        "\n",
        "def subgraph_node_2(state: SubgraphState):\n",
        "    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n",
        "\n",
        "subgraph_builder = StateGraph(SubgraphState)\n",
        "subgraph_builder.add_node(subgraph_node_1)\n",
        "subgraph_builder.add_node(subgraph_node_2)\n",
        "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
        "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
        "subgraph = subgraph_builder.compile()"
      ],
      "metadata": {
        "id": "TirCGlBJzPjX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parent graph\n",
        "class ParentState(TypedDict):\n",
        "    foo: str\n",
        "\n",
        "def node_1(state: ParentState):\n",
        "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
        "\n",
        "def node_2(state: ParentState):\n",
        "    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n",
        "    return {\"foo\": response[\"bar\"]}\n",
        "\n",
        "\n",
        "builder = StateGraph(ParentState)\n",
        "builder.add_node(\"node_1\", node_1)\n",
        "builder.add_node(\"node_2\", node_2)\n",
        "builder.add_edge(START, \"node_1\")\n",
        "builder.add_edge(\"node_1\", \"node_2\")\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ymlPdfl-6hnm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mig_Ci46vGD",
        "outputId": "ae7c4efa-6b75-413a-b08f-9604e1aee5b7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((), {'node_1': {'foo': 'hi! foo'}})\n",
            "(('node_2:9202bb38-192f-f05a-c1d2-88676d80e119',), {'subgraph_node_1': {'baz': 'baz'}})\n",
            "(('node_2:9202bb38-192f-f05a-c1d2-88676d80e119',), {'subgraph_node_2': {'bar': 'hi! foobaz'}})\n",
            "((), {'node_2': {'foo': 'hi! foobaz'}})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Schemas : 2 Level Subgraph"
      ],
      "metadata": {
        "id": "F3Ah2twp60-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grandchild graph\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.state import StateGraph, START, END\n",
        "\n",
        "class GrandChildState(TypedDict):\n",
        "    my_grandchild_key: str\n",
        "\n",
        "def grandchild_1(state: GrandChildState) -> GrandChildState:\n",
        "    # NOTE: child or parent keys will not be accessible here\n",
        "    return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}"
      ],
      "metadata": {
        "id": "sKgrSA_-6xto"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grandchild = StateGraph(GrandChildState)\n",
        "grandchild.add_node(\"grandchild_1\", grandchild_1)\n",
        "\n",
        "grandchild.add_edge(START, \"grandchild_1\")\n",
        "grandchild.add_edge(\"grandchild_1\", END)\n",
        "\n",
        "grandchild_graph = grandchild.compile()"
      ],
      "metadata": {
        "id": "6UXpagho71d-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Child graph\n",
        "class ChildState(TypedDict):\n",
        "    my_child_key: str\n",
        "\n",
        "def call_grandchild_graph(state: ChildState) -> ChildState:\n",
        "    # NOTE: parent or grandchild keys won't be accessible here\n",
        "    grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}\n",
        "    grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n",
        "    return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}"
      ],
      "metadata": {
        "id": "GLUa-W7C74f-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "child = StateGraph(ChildState)\n",
        "child.add_node(\"child_1\", call_grandchild_graph)\n",
        "child.add_edge(START, \"child_1\")\n",
        "child.add_edge(\"child_1\", END)\n",
        "child_graph = child.compile()"
      ],
      "metadata": {
        "id": "tjjuOpNp8UIX"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent graph\n",
        "class ParentState(TypedDict):\n",
        "    my_key: str\n",
        "\n",
        "def parent_1(state: ParentState) -> ParentState:\n",
        "    # NOTE: child or grandchild keys won't be accessible here\n",
        "    return {\"my_key\": \"hi \" + state[\"my_key\"]}\n",
        "\n",
        "def parent_2(state: ParentState) -> ParentState:\n",
        "    return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n",
        "\n",
        "def call_child_graph(state: ParentState) -> ParentState:\n",
        "    child_graph_input = {\"my_child_key\": state[\"my_key\"]}\n",
        "    child_graph_output = child_graph.invoke(child_graph_input)\n",
        "    return {\"my_key\": child_graph_output[\"my_child_key\"]}\n",
        ""
      ],
      "metadata": {
        "id": "hsJ8INeN8ZBv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent = StateGraph(ParentState)\n",
        "parent.add_node(\"parent_1\", parent_1)\n",
        "parent.add_node(\"child\", call_child_graph)\n",
        "parent.add_node(\"parent_2\", parent_2)\n",
        "\n",
        "parent.add_edge(START, \"parent_1\")\n",
        "parent.add_edge(\"parent_1\", \"child\")\n",
        "parent.add_edge(\"child\", \"parent_2\")\n",
        "parent.add_edge(\"parent_2\", END)\n",
        "\n",
        "parent_graph = parent.compile()\n",
        "\n",
        "for chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUcUil-V8v5u",
        "outputId": "cc62085a-dd32-4fec-b11f-5775122ce397"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((), {'parent_1': {'my_key': 'hi Bob'}})\n",
            "(('child:4bac90dd-7659-865a-926d-6311fdd65803', 'child_1:fc7b58c6-5438-845d-43b7-9f019d100918'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n",
            "(('child:4bac90dd-7659-865a-926d-6311fdd65803',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n",
            "((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n",
            "((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiagent Systems"
      ],
      "metadata": {
        "id": "r-mdKBN99p75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An agent is a system that uses LLM to decide the control flow of an application.\n",
        "\n",
        "- If agent has too many tools at its disposal and makes poor decisions about which tool to call next."
      ],
      "metadata": {
        "id": "E_r4FgQO9vbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Agent, Network Agent, Supervisor, Hierarchical, Custom."
      ],
      "metadata": {
        "id": "9ZFR6VMS-TGd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKL8ulqr81mm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}