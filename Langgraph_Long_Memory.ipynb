{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOagXX7rreVEEAt2BJmBxlP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chayan141/Langgraph-Projects/blob/main/Langgraph_Long_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Term Memory General"
      ],
      "metadata": {
        "id": "NYfyK-IHTZVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Memory : Facts about the users.\n",
        "\n",
        "Episodic Memory : Involves recalling Past Experience or events or acions. It is sometimes implemented through the fewshot prompting.\n",
        "\n",
        "Procedural Memory : Both humans and AI agents remembers set of rules to perform any tasks."
      ],
      "metadata": {
        "id": "7lOtvEmuUEc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qDwTWzimQ42U"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph trustcall langchain_core langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gemini = userdata.get('gemini_api_key')\n",
        "import os\n",
        "os.environ['gemini_api_key'] = gemini"
      ],
      "metadata": {
        "id": "bXOc7cfeYJf_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "9gXvaxieYf4V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash',api_key=gemini)"
      ],
      "metadata": {
        "id": "JBjEP33TYhb8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.config import get_store\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.store.memory import InMemoryStore"
      ],
      "metadata": {
        "id": "OcZm4Z2IYj9g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = InMemoryStore()"
      ],
      "metadata": {
        "id": "0cay5aaUYnJW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TypedDict instance\n",
        "user_profile = {\n",
        "    \"user_name\": \"Lance\",\n",
        "    \"interests\": [\"biking\", \"technology\", \"coffee\"]\n",
        "}\n",
        "user_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe9AUDbmYqkD",
        "outputId": "e7e10c7a-20cc-40ae-b96c-3429e67c287a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "namespace_for_memory = (\"1\", \"memory\")\n",
        "\n",
        "key = \"user_profile\"\n",
        "value = user_profile"
      ],
      "metadata": {
        "id": "IRjoZjs9YxxB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.put(namespace_for_memory, key, value)"
      ],
      "metadata": {
        "id": "FJd8g8bMZpKm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in store.search(namespace_for_memory):\n",
        "    print(m.dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC5Yp_UgZsMN",
        "outputId": "15127264-6150-4c84-c065-1dd42cb89553"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['1', 'memory'], 'key': 'user_profile', 'value': {'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}, 'created_at': '2025-07-04T03:06:57.182191+00:00', 'updated_at': '2025-07-04T03:06:57.182196+00:00', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "mtpjoG6uZ1aA",
        "outputId": "6491460b-f15c-4787-b109-a07598c74387"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langgraph.store.base.SearchItem"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langgraph.store.base.SearchItem</b><br/>def __init__(namespace: tuple[str, ...], key: str, value: dict[str, Any], created_at: datetime, updated_at: datetime, score: float | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langgraph/store/base/__init__.py</a>Represents an item returned from a search operation with additional metadata.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 119);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile = store.get(namespace_for_memory, key)\n",
        "profile.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVcKLrinaAi4",
        "outputId": "466498bf-ef02-4371-f9fd-e2b8cfdfd869"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'technology', 'coffee']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Long Term Memory"
      ],
      "metadata": {
        "id": "q9zreMR2bwP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_info(config: RunnableConfig):\n",
        "  \"\"\"Look up user info.\"\"\"\n",
        "    # Same as that provided to `create_react_agent`\n",
        "  store = get_store()\n",
        "  user_id = config['configurable'].get(\"user_id\")\n",
        "  user_info = store.get((\"1\",  \"memory\"), user_id)\n",
        "  return user_info.value if user_info else \"Unknown Users\""
      ],
      "metadata": {
        "id": "CxWJ9t4raKS4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(\n",
        "    model = model,\n",
        "    tools = [get_user_info],\n",
        "    store = store\n",
        ")"
      ],
      "metadata": {
        "id": "mufhhDzXa4sQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n",
        "    config={\"configurable\": {\"user_id\": \"1\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roChRVp6bKNH",
        "outputId": "9feec4df-5f71-43ac-b6a4-a790214a56c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='look up user information', additional_kwargs={}, response_metadata={}, id='67009cc5-69a8-4aa4-88b7-414982837d87'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_user_info', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0a3574db-a75c-42b9-a751-fa02f90d12d9-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': 'ce3fab0f-eb7d-49dd-936d-7a910fc48025', 'type': 'tool_call'}], usage_metadata={'input_tokens': 14, 'output_tokens': 5, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Unknown Users', name='get_user_info', id='369df786-8572-45ac-b478-bc8895bc8f63', tool_call_id='ce3fab0f-eb7d-49dd-936d-7a910fc48025'),\n",
              "  AIMessage(content='I looked up the user information and the result is \"Unknown Users\".', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--e996de24-9b12-49f1-a6f8-35d4e15c7097-0', usage_metadata={'input_tokens': 27, 'output_tokens': 15, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Long Term Memory"
      ],
      "metadata": {
        "id": "YK-pJsQGbziH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "from langgraph.config import get_store\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "class UserProfile(TypedDict):\n",
        "    \"\"\"User profile schema with typed fields\"\"\"\n",
        "    user_name: str  # The user's preferred name\n",
        "    interests: List[str]  # A list of the user's interests"
      ],
      "metadata": {
        "id": "ijQZ2-0vbjiX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_user_info(user_info: UserProfile, config: RunnableConfig):\n",
        "    \"\"\"Save user info.\"\"\"\n",
        "\n",
        "    store = get_store()\n",
        "    user_id = config['configurable'].get(\"user_id\")\n",
        "    store.put((\"2\", \"memory\"), user_id, user_info)\n",
        "    return \"Success\""
      ],
      "metadata": {
        "id": "pBrtI9yHc7H8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(\n",
        "    model = model,\n",
        "    tools = [save_user_info],\n",
        "    store = store\n",
        ")"
      ],
      "metadata": {
        "id": "MM1X0dpQdMfV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith, I have interests in bike riding\"}]},\n",
        "    config={\"configurable\": {\"user_id\": \"user_123\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfZPse1TdMdI",
        "outputId": "5a957824-a842-4024-9db7-cf736dd65dc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='My name is John Smith, I have interests in bike riding', additional_kwargs={}, response_metadata={}, id='09409924-a871-4fd2-8cc5-45fe42e99fd4'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_user_info', 'arguments': '{\"user_info\": {\"interests\": [\"bike riding\"], \"user_name\": \"John Smith\"}}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--567b2f99-9f5e-4823-aaae-08eb57ab4989-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_info': {'interests': ['bike riding'], 'user_name': 'John Smith'}}, 'id': '94c0065c-a7d1-4644-accb-7f70ff35bc64', 'type': 'tool_call'}], usage_metadata={'input_tokens': 46, 'output_tokens': 16, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='Success', name='save_user_info', id='fc9fa25a-523a-41bf-b6f0-3bfe5ef54052', tool_call_id='94c0065c-a7d1-4644-accb-7f70ff35bc64'),\n",
              "  AIMessage(content=\"OK. I've saved your user information.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a43b3f11-d27b-4f8b-8826-736379964ebc-0', usage_metadata={'input_tokens': 69, 'output_tokens': 11, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "namespace_for_memory = (\"2\", \"memory\")\n",
        "for m in store.search(namespace_for_memory):\n",
        "    print(m.dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw0YPUupdMap",
        "outputId": "8e5b108f-1f3b-4c1d-f4fb-41b4b9f81141"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['2', 'memory'], 'key': 'user_123', 'value': {'user_name': 'John Smith', 'interests': ['bike riding']}, 'created_at': '2025-07-04T03:27:16.376406+00:00', 'updated_at': '2025-07-04T03:27:16.376410+00:00', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Schema Processing Using TrustCall"
      ],
      "metadata": {
        "id": "PwhQhN9xewJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complex schemas can be difficult to extract.\n",
        "\n",
        "In addition, updating even simple schemas can pose challenges.\n",
        "\n",
        "Consider our above chatbot.\n",
        "\n",
        "We regenerated the profile schema from scratch each time we chose to save a new memory.\n",
        "\n",
        "This is inefficient, potentially wasting model tokens if the schema contains a lot of information to re-generate each time.\n",
        "\n",
        "Worse, we may loose information when regenerating the profile from scratch.\n",
        "\n",
        "Addressing these problems is the motivation for TrustCall!"
      ],
      "metadata": {
        "id": "KdlddfM4fXm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "# Conversation\n",
        "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"),\n",
        "                AIMessage(content=\"Nice to meet you, Lance.\"),\n",
        "                HumanMessage(content=\"I really like biking around San Francisco.\")]"
      ],
      "metadata": {
        "id": "pA8lVl13dMWV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trustcall import create_extractor\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "\n",
        "# Schema\n",
        "class UserProfile(BaseModel):\n",
        "    \"\"\"User profile schema with typed fields\"\"\"\n",
        "    user_name: str = Field(description=\"The user's preferred name\")\n",
        "    interests: List[str] = Field(description=\"A list of the user's interests\")"
      ],
      "metadata": {
        "id": "IGC-XmoydMTw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the extractor\n",
        "trustcall_extractor = create_extractor(\n",
        "    model,\n",
        "    tools=[UserProfile],\n",
        "    tool_choice=\"UserProfile\"\n",
        ")"
      ],
      "metadata": {
        "id": "b4vCy1a1dMRS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction\n",
        "system_msg = \"Extract the user profile from the following conversation\"\n",
        "\n",
        "# Invoke the extractor\n",
        "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
      ],
      "metadata": {
        "id": "GdQu-oRtdMOv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12300p9dMMJ",
        "outputId": "4a32f39c-b132-4ae4-c0f2-10ed7678d496"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UserProfile (1d6851df-f4dd-4eda-bcd6-01c1e81e8f17)\n",
            " Call ID: 1d6851df-f4dd-4eda-bcd6-01c1e81e8f17\n",
            "  Args:\n",
            "    interests: ['biking']\n",
            "    user_name: Lance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = result[\"responses\"]\n",
        "schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TWq3FsOdMJZ",
        "outputId": "17a76e12-9b43-4a1c-f255-c4cd925a9e04"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserProfile(user_name='Lance', interests=['biking'])]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "schema[0].model_dump()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XzqL2SFgPTx",
        "outputId": "44d441f5-6c03-4bda-aa3d-9052ec4b2fd1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking']}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"response_metadata\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC6kQxHFgTQu",
        "outputId": "c73f0436-17b7-42e6-c362-f30a966968d0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '1d6851df-f4dd-4eda-bcd6-01c1e81e8f17'}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the conversation\n",
        "updated_conversation = [HumanMessage(content=\"Hi, I'm Lance.\"),\n",
        "                        AIMessage(content=\"Nice to meet you, Lance.\"),\n",
        "                        HumanMessage(content=\"I really like biking around San Francisco.\"),\n",
        "                        AIMessage(content=\"San Francisco is a great city! Where do you go after biking?\"),\n",
        "                        HumanMessage(content=\"I really like to go to a bakery after biking.\"),]"
      ],
      "metadata": {
        "id": "FNEAWjxPgaoQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the instruction\n",
        "system_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation\"\"\""
      ],
      "metadata": {
        "id": "gc-b2S4Dgepj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
        "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation},\n",
        "                                    {\"existing\": {\"UserProfile\": schema[0].model_dump()}})\n",
        "\n",
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "781zLNe8ghc5",
        "outputId": "38914a92-b344-477b-b1fb-d5ebca5649ed"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UserProfile (3e39644d-6a29-4567-89a3-959f96d208b7)\n",
            " Call ID: 3e39644d-6a29-4567-89a3-959f96d208b7\n",
            "  Args:\n",
            "    interests: ['biking', 'bakery']\n",
            "    user_name: Lance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"response_metadata\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIg5NtSJgqfv",
        "outputId": "07e45ffa-e0ec-48aa-8659-7bf967765cbc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '3e39644d-6a29-4567-89a3-959f96d208b7'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_schema = result[\"responses\"][0]\n",
        "updated_schema.model_dump()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfdugaxeguMY",
        "outputId": "62ed925e-e0a7-4b0d-b980-9ca6ed576930"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Lance', 'interests': ['biking', 'bakery']}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Short Term Memory"
      ],
      "metadata": {
        "id": "zdNsj8GQhuXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With short-term memory enabled, long conversations can exceed the LLM's context window. Common solutions are:\n",
        "\n",
        "1. Trim Messages : Remove first or last N messages (before calling an LLM).\n",
        "\n",
        "2. Delete Messages from langgraph state permanantly.\n",
        "\n",
        "3. Summarize Messages : Summarize earlier messages in the history and replace them with a summary.\n",
        "\n",
        "4. Manage Checkpoints to store and retrieve message history."
      ],
      "metadata": {
        "id": "j27-j4sfiZ1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trim Messages : To trim message history in an agent, use pre_model_hook with the trim_messages function"
      ],
      "metadata": {
        "id": "OBMJN-b6kJ64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
        "from langgraph.graph import StateGraph, START, MessagesState\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "summarization_model = model.bind(max_tokens=128)\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "  messages = trim_messages(\n",
        "      state['messages'],\n",
        "      strategy=\"last\",\n",
        "      token_counter = count_tokens_approximately,\n",
        "      max_tokens=128,\n",
        "      start_on = \"human\",\n",
        "      end_on = (\"human\",\"tool\")\n",
        "  )\n",
        "\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "nRTO88IdjBBN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = InMemorySaver()\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"call_model\",call_model)\n",
        "builder.add_edge(START, \"call_model\")\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "7hRXG875jA-x"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
        "\n",
        "final_response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw36mm_RjA8C",
        "outputId": "f78e59a6-c660-474a-bdfc-77cdb9339fd1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "As a large language model, I don't have access to your personal information, including your name. You haven't told me your name, so I don't know it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize Messages"
      ],
      "metadata": {
        "id": "9ZQESBcZl66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langmem --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuTHXJh6mKk6",
        "outputId": "7d6b7e91-e4ab-4ad4-8074-de7ec97a1beb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/292.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langmem.short_term import SummarizationNode\n",
        "from langchain_core.messages.utils import count_tokens_approximately\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from typing import Any\n",
        "from langchain_core.messages import AnyMessage"
      ],
      "metadata": {
        "id": "01FreL7ljA54"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will keep track of our running summary in the context field (expected by the SummarizationNode)."
      ],
      "metadata": {
        "id": "6q1cDu0ynv0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(MessagesState):\n",
        "    context: dict[str, Any]"
      ],
      "metadata": {
        "id": "C8n9DZz7jA0h"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define private state that will be used only for filtering the inputs to call_model node."
      ],
      "metadata": {
        "id": "UsKQnNm7n1La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMInputState(TypedDict):\n",
        "    summarized_messages: list[AnyMessage]\n",
        "    context: dict[str, Any]\n",
        ""
      ],
      "metadata": {
        "id": "BRUkEdCbjAyD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_node = SummarizationNode(\n",
        "    token_counter=count_tokens_approximately,\n",
        "    model=summarization_model,\n",
        "    max_tokens=256,\n",
        "    max_tokens_before_summary=256,\n",
        "    max_summary_tokens=128,\n",
        ")"
      ],
      "metadata": {
        "id": "sSXtNIy3jAvg"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: LLMInputState):\n",
        "    response = model.invoke(state[\"summarized_messages\"])\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "qubh7ZnIjAs1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = InMemorySaver()\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(call_model)\n",
        "builder.add_node(\"summarize\", summarization_node)\n",
        "builder.add_edge(START, \"summarize\")\n",
        "builder.add_edge(\"summarize\", \"call_model\")\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "BW_IFs70oMfi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the graph\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)"
      ],
      "metadata": {
        "id": "tDo4qcagoPt-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_response[\"messages\"][-1].pretty_print()\n",
        "final_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uw3u8tyoZHU",
        "outputId": "b33187fa-06dd-4bce-c468-2f2c860a11b4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. You told me at the beginning of our conversation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='d522eca2-c932-4e21-93aa-92224965136e'),\n",
              "  AIMessage(content=\"Hi Bob, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--c0f66858-df80-40fe-9189-b99944dcf430-0', usage_metadata={'input_tokens': 6, 'output_tokens': 19, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='f2fb6495-ebc5-4838-b860-e53705230778'),\n",
              "  AIMessage(content='With eyes of emerald, gold, or blue,\\nA furry friend, both sleek and true.\\nA silent hunter, soft and sly,\\nA purring rumble, passing by.\\n\\nA graceful leap, a playful swat,\\nA sunbeam nap, upon the mat.\\nA curious gaze, a twitching tail,\\nA feline charm that will not fail.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--893a00fc-6694-412c-bd86-b5c680e8725d-0', usage_metadata={'input_tokens': 30, 'output_tokens': 77, 'total_tokens': 107, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='535904f5-2449-4c05-b76f-bf09335876cf'),\n",
              "  AIMessage(content='With wagging tail and happy bark,\\nA loyal friend, both light and dark.\\nA furry pal, with boundless glee,\\nA wet-nosed nudge, for you and me.\\n\\nA playful fetch, a loving gaze,\\nThrough sunny fields, in joyful haze.\\nA comforting presence, strong and true,\\nA canine love, forever new.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--f1d58f5b-dc19-4df5-b26c-525122b9849e-0', usage_metadata={'input_tokens': 113, 'output_tokens': 75, 'total_tokens': 188, 'input_token_details': {'cache_read': 0}}),\n",
              "  HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='c10a31d7-beaa-48f9-9091-de73f4d530a5'),\n",
              "  AIMessage(content='Your name is Bob. You told me at the beginning of our conversation.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--04228847-78ef-4949-b6a1-610427ad2f4c-0', usage_metadata={'input_tokens': 193, 'output_tokens': 16, 'total_tokens': 209, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in final_response[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWy8-mM6ocTK",
        "outputId": "029049ec-472a-4d3a-ef3c-8dc80c7422c2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi, my name is bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob, it's nice to meet you! How can I help you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "write a short poem about cats\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "With eyes of emerald, gold, or blue,\n",
            "A furry friend, both sleek and true.\n",
            "A silent hunter, soft and sly,\n",
            "A purring rumble, passing by.\n",
            "\n",
            "A graceful leap, a playful swat,\n",
            "A sunbeam nap, upon the mat.\n",
            "A curious gaze, a twitching tail,\n",
            "A feline charm that will not fail.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "now do the same but for dogs\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "With wagging tail and happy bark,\n",
            "A loyal friend, both light and dark.\n",
            "A furry pal, with boundless glee,\n",
            "A wet-nosed nudge, for you and me.\n",
            "\n",
            "A playful fetch, a loving gaze,\n",
            "Through sunny fields, in joyful haze.\n",
            "A comforting presence, strong and true,\n",
            "A canine love, forever new.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what's my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. You told me at the beginning of our conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context"
      ],
      "metadata": {
        "id": "7Is6NRicpmtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context Engineering is the practice of building dynamic systems that provide the right information and tools in the right format, so that a language model can plausibly accomplish a task.\n",
        "\n",
        "config : data passed at the start of a run.\n",
        "\n",
        "short_term_memory(State)\n",
        "\n",
        "long_term_memory(Store)"
      ],
      "metadata": {
        "id": "tGCXhsjCppaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human In The Loop"
      ],
      "metadata": {
        "id": "UakkNkvpq8oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approve or reject: Pause the graph before a critical step.\n",
        "\n",
        "Edit Graph State: Pause the graph to review and edit the graph state.\n",
        "\n",
        "Reviewing tool calls and validate human input."
      ],
      "metadata": {
        "id": "kOgkd4kxxWfl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0l7R6Q1-pRoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}